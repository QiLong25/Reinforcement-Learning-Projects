{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZWsnEMjaMR0"
      },
      "source": [
        "**Welcome to the second programming assignment for CS 443 RL!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfPe-_9iawIJ"
      },
      "source": [
        "This assignment will have you implementing tabular model-based methods (value iteration, policy iteration, policy evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEvS3FEgWxc6"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMgTMDy6OS2e"
      },
      "source": [
        "We will be playing on the\n",
        "Taxi environment. The environment is loaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E9zqEYpOd6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8a4605-cc55-4ce8-9930-ffe5796e6dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "taxi_env = gym.make('Taxi-v3')\n",
        "starting_state = taxi_env.reset() #must reset the environment before interacting with it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSvdrsgSH6eg"
      },
      "source": [
        "The Taxi domain was introduced in the paper \"The MAXQ Method for Hierarchical Reinforcement Learning\" by Thomas G. Dietterich (https://pdfs.semanticscholar.org/fdc7/c1e10d935e4b648a32938f13368906864ab3.pdf) in ICML 1998.\n",
        "\n",
        "The version that we will be playing is slightly simpler than the version considered in that paper. Here is a description from the open source code for gym:\n",
        "\n",
        "> Description:\n",
        "    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "> Observations:\n",
        "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
        "\n",
        "    \n",
        "    Passenger locations:\n",
        "    - 0: R(ed)\n",
        "    - 1: G(reen)\n",
        "    - 2: Y(ellow)\n",
        "    - 3: B(lue)\n",
        "    - 4: in taxi\n",
        "\n",
        "    Destinations:\n",
        "    - 0: R(ed)\n",
        "    - 1: G(reen)\n",
        "    - 2: Y(ellow)\n",
        "    - 3: B(lue)\n",
        "\n",
        "    Actions:\n",
        "    There are 6 discrete deterministic actions:\n",
        "    - 0: move south\n",
        "    - 1: move north\n",
        "    - 2: move east\n",
        "    - 3: move west\n",
        "    - 4: pickup passenger\n",
        "    - 5: drop off passenger\n",
        "\n",
        "    Rewards:\n",
        "    There is a default per-step reward of -1,\n",
        "    except for delivering the passenger, which is +20,\n",
        "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
        "\n",
        "    Rendering:\n",
        "    - blue: passenger\n",
        "    - magenta: destination\n",
        "    - yellow: empty taxi\n",
        "    - green: full taxi\n",
        "    - other letters (R, G, Y and B): locations for passengers and destinations\n",
        "    \n",
        "    state space is represented by:\n",
        "        (taxi_row, taxi_col, passenger_location, destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbcd1qAiJMrw"
      },
      "source": [
        "In order to apply the model-based method in the episodic setting, we add a terminal state (the taxi drops off the passenger at the destination) in the MDP. So we end up with 501 states in total. We provide you with an MDP class for modelling the environment. You can directly use it for all the following questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zya3GlL0Jgjy"
      },
      "outputs": [],
      "source": [
        "class MDP:\n",
        "  def __init__(self,env):\n",
        "    self.num_states = env.observation_space.n + 1\n",
        "    self.num_actions = env.action_space.n\n",
        "    self.starting_state = starting_state\n",
        "    self.action_to_plot = 1\n",
        "\n",
        "    self.R = np.zeros([self.num_states, self.num_actions])\n",
        "    self.T = np.zeros([self.num_states, self.num_actions, self.num_states])\n",
        "    self.gamma = 0.99\n",
        "\n",
        "    for state in range(self.num_states - 1):\n",
        "      for action in range(self.num_actions):\n",
        "          for transition in env.env.P[state][action]:\n",
        "              probability, next_state, reward, done = transition\n",
        "              self.R[state, action] = reward\n",
        "              self.T[state, action, next_state] = probability\n",
        "\n",
        "    for action in range(self.num_actions):\n",
        "      self.T[-1, action, -1] = 1\n",
        "\n",
        "    for state in range(self.num_states - 1):\n",
        "      state_decode = list(env.env.decode(state))\n",
        "      des_x = taxi_env.env.locs[state_decode[3]][0]\n",
        "      des_y = taxi_env.env.locs[state_decode[3]][1]\n",
        "      if (state_decode[2] == 4) and (state_decode[0] == des_x) and (state_decode[1] == des_y):\n",
        "        self.T[state, 5] = 0\n",
        "        self.T[state, 5, -1] = 1\n",
        "\n",
        "  def reset(self):\n",
        "    env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc_oI9ZO-zIb"
      },
      "source": [
        "The MDP for Taxi is created as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oac3cNkn-yQf",
        "outputId": "50338059-55d1-461f-862d-1d6dbd02eeac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "501\n",
            "6\n",
            "0.99\n",
            "(501, 6)\n",
            "(501, 6, 501)\n"
          ]
        }
      ],
      "source": [
        "obs = taxi_env.reset()\n",
        "taxi_mdp = MDP(taxi_env)\n",
        "#attributes of the MDP:\n",
        "print(taxi_mdp.num_states) #number of states\n",
        "print(taxi_mdp.num_actions) #number of actions\n",
        "print(taxi_mdp.gamma) #gamma\n",
        "print(taxi_mdp.R.shape) #reward matrix (dimension num_states X num_actions)\n",
        "print(taxi_mdp.T.shape) #transition matrix (dimension num_states x num_actions X num_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJvBKQlLD4iv"
      },
      "source": [
        "**Q1: Implement the Value Iteration algorithm and solve for $Q^\\star$**.\n",
        "\n",
        "Write a method called `value_iter(mdp)` which takes an MDP as argument and returns the optimal Q-value function ($Q^\\star$) as an `np.array` with $501 \\times 6$ elements (the number of states $\\times$ the number of actions).\n",
        "\n",
        "Do this by implementing the Value Iteration algorithm seen in class, a.k.a. by repeatedly applying the Bellman operator $\\mathcal{T}$. Run your code on the taxi environment and output the optimal Q-value function. As a stopping criterion, you can check if $\\lVert Q_n - Q_{n+1}\\rVert_\\infty < 10^{-4}$.\n",
        "\n",
        "Plot a 'learning curve' which consists of the optimal Q-value function for the `(starting state, action_to_plot)` pair at each iteration of training. More specifically, your plot should have the iteration number on the $x$-axis and $Q($starting\\_state, action\\_to\\_plot$)$ on the $y$-axis. In `value_iter` function, you can also return other variables that are useful for your plot (e.g. training values) as long as you return the final `Q_fn`.\n",
        "\n",
        "In the code above, starting_state is defined by `starting_state = taxi_env.reset()`, and `action_to_plot` is set to $1$. In your implementation, you only need to use those variables defined in the MDP class (i.e. `mdp.starting_state`, `mdp.action_to_plot`).\n",
        "\n",
        "You should expect that the Q-value function of any (state, action) pair does not exceed $20$ no matter how you choose $\\gamma$ (we provide `mdp.gamma=0.99`, but you can also try other values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr3I_PSrVVxX"
      },
      "outputs": [],
      "source": [
        "def value_iter(mdp):\n",
        "  Q_fn = np.zeros([num_states, num_actions])\n",
        "  #YOUR CODE HERE....\n",
        "  return Q_fn\n",
        "\n",
        "#might be useful for plotting\n",
        "import matplotlib.pyplot as mpl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lRBfGdbaeDE"
      },
      "source": [
        "**Q2: Implement the Policy Evaluation algorithm and solve for the Q-value function $Q^\\pi$ of a given policy $\\pi$.**\n",
        "\n",
        "Write a method called `policy_eval(mdp, policy)` which takes an MDP and a policy and then outputs the Q-value function $Q^\\pi$ (for the input policy in the input MDP) as an `np.array` with $501 \\times 6$ elements.\n",
        "\n",
        "Do this by implementing the Policy Evaluation algorithm seen in class, a.k.a. by repeatedly applying the Bellman operator $\\mathcal{T}^\\pi$. You should not assume the policy is deterministic. The format of the policy $\\pi(a|s)$ is a `(num_states) x (num_actions)` array (i.e. $501 \\times 6$). Every row of the policy matrix should be a probability distribution over actions. Run your code on the taxi environment with the policy which acts uniformly at random at every state.\n",
        "\n",
        "Plot a 'learning curve' which consists of the Q-value function for the `(starting_state, action_to_plot)` pair, i.e. $Q$(starting\\_state, action\\_to\\_plot$)$, at each iteration of training. In `policy_eval` function, you can also return other variables that are helpful for your plot (e.g. training values) as long as you return the final `Q_fn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f9alltJWwo1"
      },
      "outputs": [],
      "source": [
        "def policy_eval(mdp, policy):\n",
        "  Q_fn = np.zeros([num_states, num_actions])\n",
        "  # YOUR CODE HERE\n",
        "  return Q_fn\n",
        "\n",
        "\n",
        "#run your code on the uniform policy\n",
        "policy_unif = 1./taxi_mdp.num_actions * np.ones([taxi_mdp.num_states,taxi_mdp.num_actions])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clgdsvakk12k"
      },
      "source": [
        "**Q3: Implement Policy Iteration using your solution from Q2 as a sub-routine, and solve for the optimal policy $\\pi^\\star$**\n",
        "\n",
        "Write a method called `policy_iteration(mdp, policy)` which takes an MDP and an *initial* policy, and outputs the optimal policy $\\pi^\\star$ via successive rounds of policy improvement. The method should return the policy obtained, as a $501 \\times 6$ `np.array`.\n",
        "\n",
        "Run your code on the taxi environment with the policy which acts uniformly at random at every state as a starting policy.\n",
        "\n",
        "Plot a 'learning curve' which consists of the state-value function for the starting state (i.e. `mdp.starting_state`) for each policy obtained. You can check if/that the optimal value obtained from policy iteration is the same (or close to) the value function obtained from value iteration. You can also return other variables that is useful for your plot (e.g. training values) as long as you return the final policy.\n",
        "\n",
        "You should expect the results are exactly or almost the same in policy iteration and value iteration. Notice that we get Q-value function in `value_iter` function, and obtain state-value function in `policy_iter` function. If you want to compare the results, you need to have additional step for the conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSzaFOu5vAzR"
      },
      "outputs": [],
      "source": [
        "def policy_iter(mdp, policy_init):\n",
        "  policy = policy_init\n",
        "  #YOUR CODE HERE\n",
        "  #Use policy_eval as a sub-routine\n",
        "  return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoYJ55zCaPVA"
      },
      "source": [
        "**Instructions on converting iPython notebook to pdf**\n",
        "\n",
        "Please do not directly print the iPython notebook to pdf because it may have some issue if your code or text are too long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONx9vjT8addX"
      },
      "source": [
        "Option 1: if you run the code locally with Jupyter Notebook or Jupyter Lab, there is an option to save to pdf from the menu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOlgK0wXaVil"
      },
      "source": [
        "Option 2: if you run the code on Google colab. (You can delete the block below if you run code locally.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb8YPaYIW8k1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Here we use a script to generate pdf and save it to google drive.\n",
        "\n",
        "# After executing this cell, you will be asked to link to your GoogleDrive account.\n",
        "# Then, the pdf will be generated and saved to your GoogleDrive account and you need to go there to download;\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# install tex; first run may take several minutes\n",
        "! apt-get install texlive-xetex\n",
        "# file path and save location below are default; please change if they do not match yours\n",
        "! jupyter nbconvert --output-dir='/content/drive/MyDrive/' '/content/drive/MyDrive/TA-S24-CS443/hw2/CS443RL_Assignment2.ipynb' --to pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkgV3wFWcI2y"
      },
      "source": [
        "Also feel free to use other methods as long as the converted file visually looks good."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}